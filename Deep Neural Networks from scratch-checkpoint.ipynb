{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Deep Neural Networks from scratch in Python </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this guide we will build a deep neural network, with as many layers as you want! The network can be applied to supervised learning problem with binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Structure](img/001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "    Superscript [l] denotes a quantity associated with the lᵗʰ layer.\n",
    "    Superscript (i) denotes a quantity associated with the iᵗʰ example.\n",
    "    Lowerscript i denotes the iᵗʰ entry of a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neuron](img/002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neuron computes a linear function (z = Wx + b) followed by an activation function. We generally say that the output of a neuron is a = g(Wx + b) where g is the activation function (sigmoid, tanh, ReLU, …).\n",
    "\n",
    "#### Dataset\n",
    "Let’s assume that we have a very big dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you subtract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General methodology (building the parts of our algorithm)\n",
    "    We will follow the Deep Learning methodology to build the model:\n",
    "    \n",
    "    1. Define the model structure (such as number of input features)\n",
    "    \n",
    "    2. Initialize parameters and define hyperparameters:\n",
    "        number of iterations\n",
    "        number of layers L in the neural network\n",
    "        size of the hidden layers\n",
    "        learning rate α\n",
    "    \n",
    "    3. Loop for num_iterations:\n",
    "        Forward propagation (calculate current loss)\n",
    "        Compute cost function\n",
    "        Backward propagation (calculate current gradient)\n",
    "        Update parameters (using parameters, and grads from backprop)\n",
    "    4. Use trained parameters to predict labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization\n",
    "\n",
    "The initialization for a deeper L-layered neural network is more complicated because there are many more weight matrices and bias vectors. I provide the tables below in order to help you keep the right dimensions of the structures.\n",
    "\n",
    "![Dimension Table](img/003.png)\n",
    "\n",
    "\n",
    "##### Dimensions of weight matrix W, bias vector b and activation Z for the neural network for our example architecture\n",
    "\n",
    "![Dimension Table ABOVE arch](img/004.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(nn_architecture, seed = 3):\n",
    "    np.random.seed(seed)\n",
    "    # python dictionary containing our parameters \"W1\", \"b1\", ..., \"WL\", \"bL\"\n",
    "    parameters = {}\n",
    "    number_of_layers = len(nn_architecture)\n",
    "\n",
    "    for l in range(1, number_of_layers):\n",
    "        parameters['W' + str(l)] = np.random.randn(\n",
    "            nn_architecture[l][\"layer_size\"],\n",
    "            nn_architecture[l-1][\"layer_size\"]\n",
    "            ) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((nn_architecture[l][\"layer_size\"], 1))\n",
    "        \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters initialization using small random numbers is simple approach, but it guarantees good enough starting point for our algorithm.\n",
    "#### Remember:\n",
    "    Different initialization techniques such as Zero, Random, He or Xavier lead to different result\n",
    "    Random initialization makes sure different hidden units can learn different things (initializing all the weights to zero causes, that every neuron in each layer will learn the same thing)\n",
    "    Don’t initialize to values that are too large\n",
    "\n",
    "#### Activation functions\n",
    "    Activation functions give the neural networks non-linearity. \n",
    "    In our example, we will use sigmoid and ReLU.\n",
    "    Sigmoid outputs a value between 0 and 1 which makes it a very good choice for binary classification. You can classify the output as 0 if it is less than 0.5 and classify it as 1 if the output is more than 0.5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    S = 1 / (1 + np.exp(-Z))\n",
    "    return S\n",
    "\n",
    "def relu(Z):\n",
    "    R = np.maximum(0, Z)\n",
    "    return R\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    S = sigmoid(Z)\n",
    "    dS = S * (1 - S)\n",
    "    return dA * dS\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above code section we can see the vectorized implementation of activation functions and their derivatives.The code will be used in the further calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward propagation\n",
    "\n",
    "During forward propagation, in the forward function for a layer l you need to know what the activation function in a layer is (Sigmoid, tanh, ReLU, etc.). Given input signal from the previous layer, we compute Z and then apply selected activation function.\n",
    "\n",
    "![Forward Propagation](img/005.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters, nn_architecture):\n",
    "    forward_cache = {}\n",
    "    A = X\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    for l in range(1, number_of_layers):\n",
    "        A_prev = A \n",
    "        W = parameters['W' + str(l)]\n",
    "        b = parameters['b' + str(l)]\n",
    "        activation = nn_architecture[l][\"activation\"]\n",
    "        Z, A = linear_activation_forward(A_prev, W, b, activation)\n",
    "        forward_cache['Z' + str(l)] = Z\n",
    "        forward_cache['A' + str(l-1)] = A_prev\n",
    "    AL = A\n",
    "    return AL, forward_cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        Z = linear_forward(A_prev, W, b)\n",
    "        A = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        Z = linear_forward(A_prev, W, b)\n",
    "        A = relu(Z)\n",
    "    return Z, A\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use “cache” (Python dictionary, which contains A and Z values computed for particular layers) to pass variables computed during forward propagation to the corresponding backward propagation step. It contains useful values for backward propagation to compute derivatives.\n",
    "\n",
    "#### Loss function\n",
    "\n",
    "In order to monitor the learning process, we need to calculate the value of the cost function. We will use the below formula to calculate the cost.\n",
    "\n",
    "\n",
    "![Cost Equation](img/006.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from AL and y\n",
    "    logprobs = np.multiply(np.log(AL),Y) + np.multiply(1 - Y, np.log(1 - AL))\n",
    "    # cross-entropy cost\n",
    "    cost = - np.sum(logprobs) / m\n",
    "\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "    Backpropagation is used to calculate the gradient of the loss function with respect to the parameters. This algorithm is the recursive use of a “chain rule” known from differential calculus.\n",
    "\n",
    "    Equations used in backpropagation calculation:\n",
    "    \n",
    "  ![Backward Propagation](img/007.png)\n",
    "  \n",
    "  \n",
    "  \n",
    "### The general idea:\n",
    "The derivative of the loss function with respect to Z from lᵗʰ layer helps to calculate the derivative of the loss function with respect to A from (l-1)ᵗʰ layer (the previous layer). Then the result is used with the derivative of the activation function.\n",
    "\n",
    "##### Backward propagation for our example neural network\n",
    "\n",
    "![Backward Propagation for our model](img/008.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, parameters, forward_cache, nn_architecture):\n",
    "    grads = {}\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    dA_prev = dAL\n",
    "\n",
    "    for l in reversed(range(1, number_of_layers)):\n",
    "        dA_curr = dA_prev\n",
    "\n",
    "        activation = nn_architecture[l][\"activation\"]\n",
    "        W_curr = parameters['W' + str(l)]\n",
    "        Z_curr = forward_cache['Z' + str(l)]\n",
    "        A_prev = forward_cache['A' + str(l-1)]\n",
    "\n",
    "        dA_prev, dW_curr, db_curr = linear_activation_backward(dA_curr, Z_curr, A_prev, W_curr, activation)\n",
    "\n",
    "        grads[\"dW\" + str(l)] = dW_curr\n",
    "        grads[\"db\" + str(l)] = db_curr\n",
    "\n",
    "    return grads\n",
    "\n",
    "def linear_activation_backward(dA, Z, A_prev, W, activation):\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, Z)\n",
    "        dA_prev, dW, db = linear_backward(dZ, A_prev, W)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, Z)\n",
    "        dA_prev, dW, db = linear_backward(dZ, A_prev, W)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_backward(dZ, A_prev, W):\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update parameters\n",
    "The goal of the function is to update the parameters of the model using gradient optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(1, L+1):\n",
    "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters,nn_architecture)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)*100) + \" %\")\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full model\n",
    "The full implementation of the neural network model consists of the methods provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, nn_architecture, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    np.random.seed(1)\n",
    "    # keep track of cost\n",
    "    costs = []\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters(nn_architecture)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, forward_cache = L_model_forward(X, parameters, nn_architecture)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, parameters, forward_cache, nn_architecture)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "        costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from DNN import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_architecture = [\n",
    "    {\"layer_size\": 4, \"activation\": \"none\"}, # input layer\n",
    "    {\"layer_size\": 5, \"activation\": \"relu\"},\n",
    "    {\"layer_size\": 4, \"activation\": \"relu\"},\n",
    "    {\"layer_size\": 3, \"activation\": \"relu\"},\n",
    "    {\"layer_size\": 1, \"activation\": \"sigmoid\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = L_layer_model(train_x.T, train_y.T, nn_architecture, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = predict(train_x.T, train_y.T, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = predict(test_x.T, test_y.T, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In order to make a prediction, you only need to run a full forward propagation using the received weight matrix and a set of test data.\n",
    "##### We can modify nn_architecture in Snippet 1 to build a neural network with a different number of layers and sizes of the hidden layers.\n",
    "##### Moreover, prepare the correct implementation of the activation functions and their derivatives (Snippet 2).\n",
    "##### The implemented functions can be used to modify linear_activation_forward method in Snippet 3 and linear_activation_backward method in Snippet 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further improvements\n",
    "    We can face the “overfitting” problem if the training dataset is not big enough. It means that the learned network doesn’t generalize to new examples that it has never seen. We can use regularization methods such as L2 regularization (it consists of appropriately modifying our cost function) or dropout ( it randomly shuts down some neurons in each iteration).\n",
    "\n",
    "    We used Gradient Descent to update the parameters and minimize the cost. You can learn more advanced optimization methods that can speed up learning and even get you to a better final value for the cost function for example:\n",
    "\n",
    "    Mini-batch gradient descent\n",
    "    Momentum\n",
    "    Adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "    Course Name: Neural Networks and Deep Learning\n",
    "    Instructor: Andrew ng\n",
    "    From:deeplearning.ai\n",
    "    Platform: Coursera\n",
    "    Course URL: https://www.coursera.org/learn/neural-networks-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thanks,\n",
    "### Shubham Sagar\n",
    "### Follow me at: www.instagaram.com/shubhamthrills  / https://github.com/shubhamthrills"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
